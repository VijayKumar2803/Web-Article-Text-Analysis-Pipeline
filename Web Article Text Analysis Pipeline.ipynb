{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34199,"status":"ok","timestamp":1715930264154,"user":{"displayName":"Vijay Kumar","userId":"14824207336965196146"},"user_tz":-330},"id":"piwAUO1T5LV5","outputId":"e4ca8c63-3c2b-4d5c-8303-1473188d430a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","source":["pip install requests beautifulsoup4 nltk pandas openpyxl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O0OVogDaPCIU","executionInfo":{"status":"ok","timestamp":1715930280711,"user_tz":-330,"elapsed":8667,"user":{"displayName":"Vijay Kumar","userId":"14824207336965196146"}},"outputId":"f7da1253-68d6-49ab-98ea-90ec941bb6e9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","\n","def extract_article_text(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    # Extract the title and article text (adjust selectors as needed)\n","    title = soup.find('h1').get_text(strip=True)\n","    paragraphs = soup.find_all('p')\n","    article_text = '\\n'.join([para.get_text(strip=True) for para in paragraphs])\n","\n","    return title + \"\\n\" + article_text\n","\n","def save_article_text(url_id, text):\n","    with open(f'extracted_articles/{url_id}.txt', 'w', encoding='utf-8') as file:\n","        file.write(text)\n","\n","def main():\n","    # Read the input URLs\n","    input_df = pd.read_excel('/content/drive/MyDrive/Test Assignment/20211030 Test Assignment/Input.xlsx')\n","\n","    # Ensure the output directory exists\n","    os.makedirs('extracted_articles', exist_ok=True)\n","\n","    # Extract and save article text for each URL\n","    for _, row in input_df.iterrows():\n","        url_id = row['URL_ID']\n","        url = row['URL']\n","        try:\n","            article_text = extract_article_text(url)\n","            save_article_text(url_id, article_text)\n","        except Exception as e:\n","            print(f\"Failed to extract {url_id} - {url}: {e}\")\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFszRLU5RV5y","executionInfo":{"status":"ok","timestamp":1715930433492,"user_tz":-330,"elapsed":93195,"user":{"displayName":"Vijay Kumar","userId":"14824207336965196146"}},"outputId":"d21db5d5-2e90-4750-e516-f06ea0678f7c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to extract blackassign0036 - https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 'NoneType' object has no attribute 'get_text'\n","Failed to extract blackassign0049 - https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 'NoneType' object has no attribute 'get_text'\n"]}]},{"cell_type":"code","source":["pip install textstat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbMy0fesVDt9","executionInfo":{"status":"ok","timestamp":1715930439844,"user_tz":-330,"elapsed":6360,"user":{"displayName":"Vijay Kumar","userId":"14824207336965196146"}},"outputId":"e0771f44-2868-4b8d-94fd-b58e43efdfbd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textstat\n","  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m92.2/105.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyphen (from textstat)\n","  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyphen, textstat\n","Successfully installed pyphen-0.15.0 textstat-0.7.3\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import textstat\n","import pandas as pd\n","import os\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load stopwords\n","stopwords = set(stopwords.words('english'))\n","\n","# Function to read files with encoding handling\n","def read_file_with_encoding_handling(file_path):\n","    for encoding in ['utf-8', 'latin-1']:\n","        try:\n","            with open(file_path, 'r', encoding=encoding) as file:\n","                return file.read().split()\n","        except UnicodeDecodeError:\n","            continue\n","    raise ValueError(f\"Failed to read file with known encodings: {file_path}\")\n","\n","# Load positive and negative words\n","positive_words = set(read_file_with_encoding_handling('/content/drive/MyDrive/Test Assignment/20211030 Test Assignment/MasterDictionary/positive-words.txt'))\n","negative_words = set(read_file_with_encoding_handling('/content/drive/MyDrive/Test Assignment/20211030 Test Assignment/MasterDictionary/negative-words.txt'))\n","\n","def compute_variables(text):\n","    words = word_tokenize(text.lower())\n","    sentences = sent_tokenize(text)\n","\n","    # Positive and negative score\n","    positive_score = sum(1 for word in words if word in positive_words)\n","    negative_score = sum(1 for word in words if word in negative_words)\n","\n","    # Polarity and subjectivity\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n","\n","    # Sentence lengths and complexity\n","    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n","    complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n","    percentage_of_complex_words = len(complex_words) / len(words)\n","    fog_index = textstat.gunning_fog(text)\n","    avg_words_per_sentence = len(words) / len(sentences)\n","\n","    # Other metrics\n","    complex_word_count = len(complex_words)\n","    word_count = len(words)\n","    syllable_per_word = sum(textstat.syllable_count(word) for word in words) / len(words)\n","    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n","    avg_word_length = sum(len(word) for word in words) / len(words)\n","\n","    return {\n","        'positive_score': positive_score,\n","        'negative_score': negative_score,\n","        'polarity_score': polarity_score,\n","        'subjectivity_score': subjectivity_score,\n","        'avg_sentence_length': avg_sentence_length,\n","        'percentage_of_complex_words': percentage_of_complex_words,\n","        'fog_index': fog_index,\n","        'avg_words_per_sentence': avg_words_per_sentence,\n","        'complex_word_count': complex_word_count,\n","        'word_count': word_count,\n","        'syllable_per_word': syllable_per_word,\n","        'personal_pronouns': personal_pronouns,\n","        'avg_word_length': avg_word_length\n","    }\n","\n","def main():\n","    input_df = pd.read_excel('/content/drive/MyDrive/Test Assignment/20211030 Test Assignment/Input.xlsx')\n","    output_df = pd.read_excel('/content/drive/MyDrive/Test Assignment/20211030 Test Assignment/Output Data Structure.xlsx')\n","\n","    results = []\n","\n","    for _, row in input_df.iterrows():\n","        url_id = row['URL_ID']\n","        file_path = f'extracted_articles/{url_id}.txt'\n","        if os.path.exists(file_path):\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                text = file.read()\n","                variables = compute_variables(text)\n","                results.append([url_id, row['URL'], *variables.values()])\n","        else:\n","            print(f\"File not found: {file_path}\")\n","\n","    # Save results to DataFrame\n","    result_df = pd.DataFrame(results, columns=output_df.columns)\n","    result_df.to_excel('/content/drive/MyDrive/Test Assignment/20211030 Test Assignment/Output Data Structure.xlsx', index=False)\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUZ0vyUiUFVk","executionInfo":{"status":"ok","timestamp":1715930452597,"user_tz":-330,"elapsed":12761,"user":{"displayName":"Vijay Kumar","userId":"14824207336965196146"}},"outputId":"46727e14-252c-4d34-e9ad-4a0a22cd6a78"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["File not found: extracted_articles/blackassign0036.txt\n","File not found: extracted_articles/blackassign0049.txt\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9fjI2vB6VAKi"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}